#model_checkpoint:
#  _target_: pytorch_lightning.callbacks.ModelCheckpoint
#  monitor: "val/f1"   # name of the logged metric which determines when model is improving
#  save_top_k: 1       # save k best models (determined by above metric)
#  save_last: False     # additionally always save model from last epoch
#  mode: "max"         # can be "max" or "min"
#  verbose: False
#  dirpath: "checkpoints/"
#  filename: "best"

lr_monitor:
  _target_: pytorch_lightning.callbacks.LearningRateMonitor
  logging_interval: step


